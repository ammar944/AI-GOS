---
phase: 15-rag-foundation
plan: 03
type: execute
---

<objective>
Create retrieval service, Q&A agent, and chat API endpoint for Blueprint Chat.

Purpose: Enable users to ask questions about their blueprints using RAG-powered responses.
Output: Working chat API that answers questions using vector search and Claude Sonnet.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-rag-foundation/DISCOVERY.md
@.planning/phases/15-rag-foundation/15-01-SUMMARY.md
@.planning/phases/15-rag-foundation/15-02-SUMMARY.md
@.planning/docs/Blueprint_AI_Chat_RAG_Specification_OpenRouter.md

**Codebase constraints:**
- Use existing OpenRouterClient for chat completions
- Use Claude Sonnet (MODELS.CLAUDE_SONNET) for Q&A responses
- Follow existing API route patterns in src/app/api/
- Use Supabase RPC for vector search (match_blueprint_chunks)

**Prior decisions:**
- Phase 15 is Q&A only - no intent routing, edit, or explain agents
- Basic chat with RAG context retrieval
- Claude Sonnet for balanced quality/cost

@src/lib/openrouter/client.ts
@src/lib/chat/types.ts
@src/lib/chat/embeddings.ts
@src/lib/supabase/server.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create retrieval service</name>
  <files>src/lib/chat/retrieval.ts</files>
  <action>
Create the retrieval service that queries pgvector for relevant chunks:

```typescript
// src/lib/chat/retrieval.ts
import { createServerClient } from '@/lib/supabase/server';
import { generateEmbedding } from './embeddings';
import { BlueprintChunk, BlueprintSection } from './types';

export interface RetrievalOptions {
  blueprintId: string;
  query: string;
  matchThreshold?: number;  // Default 0.7
  matchCount?: number;      // Default 5
  sectionFilter?: BlueprintSection;
}

export interface RetrievalResult {
  chunks: BlueprintChunk[];
  embeddingCost: number;
}

/**
 * Retrieve relevant chunks from a blueprint using vector similarity search
 */
export async function retrieveRelevantChunks(
  options: RetrievalOptions
): Promise<RetrievalResult> {
  const {
    blueprintId,
    query,
    matchThreshold = 0.7,
    matchCount = 5,
    sectionFilter = null,
  } = options;

  // 1. Generate query embedding
  const queryEmbedding = await generateEmbedding(query);

  // 2. Call Supabase RPC for vector search
  const supabase = await createServerClient();
  const { data, error } = await supabase.rpc('match_blueprint_chunks', {
    query_embedding: queryEmbedding,
    p_blueprint_id: blueprintId,
    match_threshold: matchThreshold,
    match_count: matchCount,
    section_filter: sectionFilter,
  });

  if (error) {
    throw new Error(`Retrieval failed: ${error.message}`);
  }

  // 3. Map to BlueprintChunk type (snake_case to camelCase)
  const chunks: BlueprintChunk[] = (data || []).map((row: any) => ({
    id: row.id,
    blueprintId,
    section: row.section as BlueprintSection,
    fieldPath: row.field_path,
    content: row.content,
    contentType: row.content_type,
    metadata: row.metadata,
    embedding: [], // Not returned from search
    similarity: row.similarity,
    createdAt: new Date(),
    updatedAt: new Date(),
  }));

  return {
    chunks,
    embeddingCost: 0.00002 * (query.length / 4), // Rough estimate
  };
}

/**
 * Build context string from retrieved chunks for LLM prompt
 */
export function buildContextFromChunks(chunks: BlueprintChunk[]): string {
  if (chunks.length === 0) {
    return 'No relevant context found in the blueprint.';
  }

  return chunks
    .map((chunk, i) => {
      const similarity = chunk.similarity
        ? ` (relevance: ${(chunk.similarity * 100).toFixed(0)}%)`
        : '';
      return `[${i + 1}] ${chunk.metadata.sectionTitle} - ${chunk.metadata.fieldDescription}${similarity}:
${chunk.content}`;
    })
    .join('\n\n');
}
```
  </action>
  <verify>
`npx tsc --noEmit src/lib/chat/retrieval.ts`
  </verify>
  <done>Retrieval service queries pgvector and builds context for LLM</done>
</task>

<task type="auto">
  <name>Task 2: Create Q&A agent</name>
  <files>src/lib/chat/agents/qa-agent.ts</files>
  <action>
Create the Q&A agent that answers questions using RAG context:

```typescript
// src/lib/chat/agents/qa-agent.ts
import { createOpenRouterClient, MODELS } from '@/lib/openrouter/client';
import { BlueprintChunk } from '../types';
import { buildContextFromChunks } from '../retrieval';

export interface QARequest {
  query: string;
  chunks: BlueprintChunk[];
  chatHistory?: { role: 'user' | 'assistant'; content: string }[];
}

export interface QAResponse {
  answer: string;
  sources: BlueprintChunk[];
  confidence: 'high' | 'medium' | 'low';
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  cost: number;
}

const QA_SYSTEM_PROMPT = `You are an expert assistant for Strategic Blueprint documents.
Your role is to answer questions about the blueprint accurately and helpfully.

RULES:
1. Answer using ONLY the provided context - do not make up information
2. If the answer isn't in the context, clearly say "I don't have that information in the blueprint"
3. Be specific and reference actual data from the blueprint
4. If multiple chunks are relevant, synthesize them into a coherent answer
5. Keep answers concise but complete
6. When referencing specific data, mention which section it comes from

CONTEXT SECTIONS:
- Industry Market Overview: Market landscape, pain points, psychological drivers
- ICP Analysis & Validation: ICP viability and validation
- Offer Analysis & Viability: Offer strength scores and recommendations
- Competitor Analysis: Competitor profiles and gaps
- Cross-Analysis Synthesis: Strategic recommendations and next steps`;

/**
 * Answer a question about the blueprint using RAG context
 */
export async function answerQuestion(request: QARequest): Promise<QAResponse> {
  const { query, chunks, chatHistory = [] } = request;

  const context = buildContextFromChunks(chunks);

  // Build messages with context
  const messages = [
    { role: 'system' as const, content: QA_SYSTEM_PROMPT },
    ...chatHistory.slice(-6).map(m => ({
      role: m.role as 'user' | 'assistant',
      content: m.content,
    })),
    {
      role: 'user' as const,
      content: `## Blueprint Context:
${context}

## Question:
${query}

Answer the question based on the blueprint data above.`,
    },
  ];

  const client = createOpenRouterClient();
  const response = await client.chat({
    model: MODELS.CLAUDE_SONNET,
    messages,
    temperature: 0.3,
    maxTokens: 1024,
  });

  // Determine confidence based on chunk relevance
  const avgSimilarity = chunks.length > 0
    ? chunks.reduce((sum, c) => sum + (c.similarity || 0), 0) / chunks.length
    : 0;

  const confidence: 'high' | 'medium' | 'low' =
    avgSimilarity > 0.85 ? 'high' :
    avgSimilarity > 0.7 ? 'medium' : 'low';

  return {
    answer: response.content,
    sources: chunks,
    confidence,
    usage: response.usage,
    cost: response.cost,
  };
}
```
  </action>
  <verify>
`npx tsc --noEmit src/lib/chat/agents/qa-agent.ts`
  </verify>
  <done>Q&A agent uses RAG context to answer questions via Claude Sonnet</done>
</task>

<task type="auto">
  <name>Task 3: Create chat API endpoint</name>
  <files>src/app/api/blueprint/[id]/chat/route.ts</files>
  <action>
Create the chat API endpoint following Next.js App Router patterns:

```typescript
// src/app/api/blueprint/[id]/chat/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { retrieveRelevantChunks } from '@/lib/chat/retrieval';
import { answerQuestion, QAResponse } from '@/lib/chat/agents/qa-agent';

interface ChatRequest {
  message: string;
  conversationId?: string;
  chatHistory?: { role: 'user' | 'assistant'; content: string }[];
}

interface ChatResponse {
  conversationId: string;
  response: string;
  sources: {
    chunkId: string;
    section: string;
    fieldPath: string;
    similarity: number;
  }[];
  confidence: 'high' | 'medium' | 'low';
  metadata: {
    tokensUsed: number;
    cost: number;
    processingTime: number;
  };
}

export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ id: string }> }
) {
  const startTime = Date.now();
  const { id: blueprintId } = await params;

  try {
    const body: ChatRequest = await request.json();

    if (!body.message?.trim()) {
      return NextResponse.json(
        { error: 'Message is required' },
        { status: 400 }
      );
    }

    const conversationId = body.conversationId || crypto.randomUUID();

    // 1. Retrieve relevant chunks
    const { chunks, embeddingCost } = await retrieveRelevantChunks({
      blueprintId,
      query: body.message,
      matchCount: 5,
      matchThreshold: 0.65, // Slightly lower for better recall
    });

    // 2. Generate answer using Q&A agent
    const qaResult = await answerQuestion({
      query: body.message,
      chunks,
      chatHistory: body.chatHistory,
    });

    // 3. Build response
    const response: ChatResponse = {
      conversationId,
      response: qaResult.answer,
      sources: chunks.map(c => ({
        chunkId: c.id,
        section: c.section,
        fieldPath: c.fieldPath,
        similarity: c.similarity || 0,
      })),
      confidence: qaResult.confidence,
      metadata: {
        tokensUsed: qaResult.usage.totalTokens,
        cost: qaResult.cost + embeddingCost,
        processingTime: Date.now() - startTime,
      },
    };

    return NextResponse.json(response);

  } catch (error) {
    console.error('Chat error:', error);

    const message = error instanceof Error ? error.message : 'Unknown error';
    return NextResponse.json(
      { error: 'Failed to process chat message', details: message },
      { status: 500 }
    );
  }
}
```

Note: This is Phase 15's basic Q&A. Intent routing and other agents come in Phase 16-17.
  </action>
  <verify>
`npx tsc --noEmit src/app/api/blueprint/[id]/chat/route.ts`
  </verify>
  <done>POST /api/blueprint/[id]/chat endpoint handles Q&A requests</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npx tsc --noEmit` passes with no errors
- [ ] Retrieval service compiles and has correct types
- [ ] Q&A agent compiles and uses Claude Sonnet
- [ ] Chat API endpoint compiles and follows Next.js patterns
- [ ] All imports resolve correctly
</verification>

<success_criteria>
- Retrieval service queries pgvector via Supabase RPC
- Q&A agent generates answers using RAG context
- Chat API endpoint accepts POST requests with message
- Response includes sources and confidence level
- All TypeScript types are correct
</success_criteria>

<output>
After completion, create `.planning/phases/15-rag-foundation/15-03-SUMMARY.md`
</output>
