---
phase: 18-chat-streaming
plan: 01
type: execute
---

<objective>
Add SSE streaming to Blueprint Chat for real-time response display.

Purpose: Improve perceived responsiveness by showing tokens as they're generated instead of waiting for full completion.
Output: Streaming chat responses with real-time token display in the UI.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Codebase constraints:**
- Next.js 16 App Router with API routes
- OpenRouter uses OpenAI-compatible streaming format (stream: true, SSE with delta chunks)
- Existing OpenRouterClient at src/lib/openrouter/client.ts
- Chat API at src/app/api/blueprint/[id]/chat/route.ts (returns full JSON response)
- Chat UI at src/components/chat/blueprint-chat.tsx (uses fetch, awaits full response)

**Relevant source files:**
@src/lib/openrouter/client.ts
@src/app/api/blueprint/[id]/chat/route.ts
@src/components/chat/blueprint-chat.tsx
@src/components/chat/chat-message.tsx

**OpenRouter streaming format (verified):**
- Set `stream: true` in request body
- Response is SSE with `data: {...}` lines
- Each chunk has `choices[0].delta.content` (not `message.content`)
- Stream ends with `data: [DONE]`
- Keep-alive comments (`: comment`) should be ignored
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add chatStream method to OpenRouterClient</name>
  <files>src/lib/openrouter/client.ts</files>
  <action>
Add a new `chatStream()` method to OpenRouterClient that:
1. Takes same ChatCompletionOptions but adds `stream: true` to request body
2. Returns an async generator that yields string chunks (delta.content)
3. Uses native fetch with response.body (ReadableStream)
4. Parses SSE format: split on `data: `, parse JSON, extract delta.content
5. Handle `[DONE]` termination signal
6. Ignore SSE comments (lines starting with `:`)
7. Handle errors mid-stream by yielding error message

Signature:
```typescript
async *chatStream(options: ChatCompletionOptions): AsyncGenerator<string, void, unknown>
```

Do NOT add response_format for streaming (JSON mode incompatible with streaming).
Do NOT try to track usage/cost during streaming (not available until stream ends).
  </action>
  <verify>TypeScript compiles without errors: `npx tsc --noEmit`</verify>
  <done>chatStream method exists and compiles, returns AsyncGenerator<string></done>
</task>

<task type="auto">
  <name>Task 2: Add streaming chat API endpoint</name>
  <files>src/app/api/chat/blueprint/stream/route.ts</files>
  <action>
Create new streaming endpoint at `/api/chat/blueprint/stream` that:
1. Accepts same request body as existing chat endpoint (message, blueprint, chatHistory)
2. For Q&A intent: stream the response using SSE
3. For edit/explain intents: return full JSON (edits need structured data for confirmation)
4. Use TransformStream or ReadableStream for SSE response
5. Set headers: `Content-Type: text/event-stream`, `Cache-Control: no-cache`, `Connection: keep-alive`
6. Format: `data: {"content": "token"}\n\n` for each chunk
7. End with `data: {"done": true, "metadata": {...}}\n\n`

Key implementation:
- Create inline Q&A logic (don't import answerQuestion - need streaming variant)
- Use openRouterClient.chatStream() for the LLM call
- Include intent classification before streaming decision
- For non-streaming intents, return regular JSON response (not SSE)
  </action>
  <verify>Endpoint responds with SSE stream: `curl -X POST http://localhost:3000/api/chat/blueprint/stream -H "Content-Type: application/json" -d '{"message":"test","blueprint":{}}' -N 2>&1 | head -5`</verify>
  <done>Streaming endpoint returns SSE for Q&A, JSON for edit intents</done>
</task>

<task type="auto">
  <name>Task 3: Update Chat UI for streaming consumption</name>
  <files>src/components/chat/blueprint-chat.tsx</files>
  <action>
Update BlueprintChat component to:
1. Use streaming endpoint for Q&A messages
2. Process SSE stream with EventSource or fetch + ReadableStream reader
3. Update message content incrementally as tokens arrive
4. Show typing indicator while streaming (reuse existing isLoading state)
5. Handle stream completion and extract metadata
6. Fallback to non-streaming for edit proposals (they need structured response)

Implementation approach:
- Add helper function `streamChat()` that returns AsyncGenerator<string>
- In handleSubmit, detect if response is streaming (Content-Type check) or JSON
- For streaming: create assistant message immediately, update content as chunks arrive
- For JSON (edits): use existing flow

Keep existing edit confirmation flow unchanged - only Q&A responses stream.
  </action>
  <verify>
1. Run dev server: `npm run dev`
2. Open chat, ask a question
3. Observe tokens appearing incrementally (not all at once)
  </verify>
  <done>Chat displays streaming responses for Q&A, edit proposals still work with confirmation flow</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `npm run build` succeeds without errors
- [ ] `npx tsc --noEmit` passes
- [ ] Q&A messages stream in real-time (tokens appear incrementally)
- [ ] Edit requests still return structured response with confirmation UI
- [ ] No console errors during streaming
- [ ] Chat remains functional for all intent types (question, edit, explain)
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Q&A responses stream token-by-token in the UI
- Edit proposals still work with diff preview and confirm/cancel
- No regression in existing chat functionality
</success_criteria>

<output>
After completion, create `.planning/phases/18-chat-streaming/18-01-SUMMARY.md`:

# Phase 18 Plan 01: Chat Streaming Summary

**[Substantive one-liner - what shipped]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Phase complete, ready for next milestone
</output>
